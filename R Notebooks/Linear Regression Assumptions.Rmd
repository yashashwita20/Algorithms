# Linear Regression Assumptions

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE, message=FALSE)
```

Loading Libraries
```{r}
library(MASS) 
library(car)
require(ellipse)
library(corrplot)
```


Using Boston dataset in package MASS. Fitting a linear model with medv as response, omitting all the discrete predictor variables.
```{r}
data(Boston)
data <- Boston[,!names(Boston) %in% c("chas","rad")] 
model = lm(medv ~ .,data)
summary(model)
model$coefficients
```

## Mean 0 Assumption
```{r fig38}
plot(model,which=1)
```

There is an obvious curvature in the above plot which explains that it is deviating from Mean 0 assumption. 

Plotting the residuals plots to check where the non linearity is coming from:
```{r fig39}
residualPlots(model)
```

We can observe that rm and lstat are clearly non linear.

## Homoscedasticity
```{r fig40}
plot(model, which=1)
```


Observe that the variance is not equal, it is not complying with the homoscedasticity assumption

## Normality
```{r fig41}
qqPlot(model$residuals)
```

Normality assumption is being violated as the points are clearly deviated from the diagonal line in the QQ Plot.

## Outliers in predictor via hatvalues
```{r fig42}
# h is the threshold for suspects
plot(hatvalues(model), type='h', col="blue", ylab="Hat Values", main="Hat values")
n = dim(data)[1]
p = dim(data)[2]-1
abline(h = 2 * (p + 1) / n, lty = 2,col = 'darkred')
```

The points above the threshold have more leverage and are the suspected outliers in predictor.

## Outliers in response via externally studentized residuals

```{r fig43}
# h is the threshold for suspects
plot(abs(rstudent(model)), type='h', col="blue",
     ylab="Externally Studentized Residuals (in absolute value)",
     main="Externally Studentized Residuals (in absolute value)")
abline(h = qt(.95, n-p-2), lty=2) # threshold for suspects
```

The points above the threshold have more leverage and are the suspected outliers in response.

## Influential observations via Cookâ€™s distance

```{r fig44}
#suspects are highlighted on the graph
par(mfrow=c(1,1))
plot(model, which=4, col="blue", lwd=2) 
abline(h = 4/n, lty=2) #threshold for suspects
```


## Influential observations via DFBETAS
```{r fig45}
par(mfrow=c(3,4)) 
for (j in 1:12) {
  plot(abs(dfbetas(model)[,j]), col=4, type='h', ylab='DFBETAS', xlab = names(data)[j])
    abline(h = 2/sqrt(n), lty=2) # threshold for suspects 
}
```

An observation can be considered to be influential in estimating a given parameter if it has a DBETAS value greater than a threshold value. Thus, the points that lie above the threshold in the plots are influential.

## Influential observations via DFFITS

```{r fig46}
par(mfrow=c(1,1))
plot(abs(dffits(model)), typ='h', col=4, ylab='DFFITS') 
abline(h = 2*sqrt(p/n), lty=2) # threshold for suspects
```

DFFITS - difference in fits, it quantifies the number of standard deviations that the fitted value changes when the nth point in the data is left out. Observations that have DFFITS values greater than a threshold in the plot are influential.

## Multicolinearity

```{r}
#correlation between all predictors
df_cor = round( cor(data) , 2)
df_cor
```

Correlation heatmap

```{r fig47}
#the higher the absolute value, higher the correlation.
#the -/+ sign tells us if they are negatively or positively correlated.
#if two predictors are correlated, they increase the overall variance in the model.
corrplot.mixed(df_cor)
```

Here nox and indus have the most correlation.

## Checking for multicolinearity via variance inflation factors (VIF)

```{r fig48}
# VIF tells us what fraction of the variance of one predictor can be explained
# by the other predictors.
# High vif tells us that the predictor can be correlated to one or more predictors
# in the model.
vif(model)
plot(vif(model), type='h', col=4, lwd=3)
abline(h = 5, lty=2)
```

Here nox, dis, indus, tax could be the suspected predictors based on their high vif values.