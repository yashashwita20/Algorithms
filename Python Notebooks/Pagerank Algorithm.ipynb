{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJCEyyf3IlRw"
   },
   "source": [
    "**Keywords**: Pagerank, Power Method\n",
    "\n",
    "**About the dataset**: \\\n",
    "*DBpedia* (from \"DB\" for \"database\") is a project aiming to extract structured content from the information created in the Wikipedia project. This structured information is made available on the World Wide Web. DBpedia allows users to semantically query relationships and properties of Wikipedia resources, including links to other related datasets. for more info, see: https://en.wikipedia.org/wiki/DBpedia. \\\n",
    "We will download two files from the data respository:\n",
    "* The first file -- **redirects_en.nt.bz2** -- contains redirects link for a page. Let A redirect to B and B redirect to C. Then we will replace article A by article C wherever needed.\n",
    "* The second file -- **page_links_en.nt.bz2** -- contains pagelinks which are links within an article to other wiki article.\n",
    "\n",
    "Note that the data is both files is a list of lines which can be split into 4 parts:\n",
    "* The link to first article.\n",
    "* Whether it is a redirect, or a pagelink.\n",
    "* The link to second article.\n",
    "* A fullstop.\n",
    "\n",
    "#### <font color=\"red\">Note:</font> Any line which cannot be split into 4 parts is skipped from consideration.\n",
    "\n",
    "**Agenda**:\n",
    "* In this notebook, I will be implementing the [*google pagerank algorithm*](https://towardsdatascience.com/pagerank-algorithm-fully-explained-dc794184b4af) to determine the most important articles.\n",
    "* This will be done by computing the principal eigenvector of the article-article graph adjacency matrix.\n",
    "* I will be applying the *power method* to extract the principal eigenvector from the adjacency matrix. \n",
    "* Using the computed eigenvector, we can assign each article a eigenvector-centrality score. Then we can determine the most important articles.\n",
    "\n",
    "**Environment**:\n",
    "Ensure following libraries are installed\n",
    "- sklearn\n",
    "- numpy\n",
    "\n",
    "Also ensure that you have around **4 GB** of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2ivsajX4yuy"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LfVmAEtTfNXf"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pickle\n",
    "from bz2 import BZ2File\n",
    "import bz2\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pprint\n",
    "from time import time\n",
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "import scipy.sparse as sparse\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OARGSLGW50J8",
    "outputId": "59435ee0-e49f-45c6-f68a-d9a74e0991dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from 'http://downloads.dbpedia.org/3.5.1/en/redirects_en.nt.bz2', please wait...\n",
      "\n",
      "Downloading data from 'http://downloads.dbpedia.org/3.5.1/en/page_links_en.nt.bz2', please wait...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# downloading the dataset and store files in local\n",
    "\n",
    "# dbpedia download urls\n",
    "redirects_url = \"http://downloads.dbpedia.org/3.5.1/en/redirects_en.nt.bz2\"\n",
    "page_links_url = \"http://downloads.dbpedia.org/3.5.1/en/page_links_en.nt.bz2\"\n",
    "\n",
    "# extracting the file-names from the urls. Needed to load the files later\n",
    "redirects_filename = redirects_url.rsplit(\"/\", 1)[1] # redirects_en.nt.bz2 ~ 59MB\n",
    "page_links_filename = page_links_url.rsplit(\"/\", 1)[1] # page_links_en.nt.bz2 ~ 850MB\n",
    "\n",
    "resources = [\n",
    "    (redirects_url, redirects_filename),\n",
    "    (page_links_url, page_links_filename),\n",
    "]\n",
    "\n",
    "# downloading the files\n",
    "# this will take some time\n",
    "for url, filename in resources:\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"Downloading data from '%s', please wait...\" % url)\n",
    "        opener = urlopen(url)\n",
    "        open(filename, \"wb\").write(opener.read())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7Rvb3xDT51Ky"
   },
   "outputs": [],
   "source": [
    "# loading the data from the downloaded files\n",
    "\n",
    "#reading redirects file\n",
    "redirects_file = bz2.open(redirects_filename, mode='rt')\n",
    "redirects_data = redirects_file.readlines()\n",
    "redirects_file.close()\n",
    "\n",
    "# pagelinks data has 119M entries\n",
    "# We will only consider the first 5M for this question\n",
    "pagelinks_file = bz2.open(page_links_filename, mode='rt')\n",
    "pagelinks_data = [next(pagelinks_file) for x in range(5000000)] \n",
    "pagelinks_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZGcZQ_SEhuEv",
    "outputId": "6766c4cd-0774-485d-f0ef-9669758544d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of entries in redirects: 4082533\n",
      "A couple of examples from redirects:\n",
      "['<http://dbpedia.org/resource/Proper_superset> <http://dbpedia.org/property/redirect> <http://dbpedia.org/resource/Subset> .\\n', '<http://dbpedia.org/resource/Jean_Paul_Sartre> <http://dbpedia.org/property/redirect> <http://dbpedia.org/resource/Jean-Paul_Sartre> .\\n']\n",
      "\n",
      "\n",
      "The number of entries in pagelinks: 5000000\n",
      "A couple of examples from pagelinks:\n",
      "['<http://dbpedia.org/resource/Antipope> <http://dbpedia.org/property/wikilink> <http://dbpedia.org/resource/Council_of_Constance> .\\n', '<http://dbpedia.org/resource/Antipope> <http://dbpedia.org/property/wikilink> <http://dbpedia.org/resource/Pope_Alexander_V> .\\n']\n"
     ]
    }
   ],
   "source": [
    "# looking at the size of the data and some examples\n",
    "print ('The number of entries in redirects:', len(redirects_data))\n",
    "print ('A couple of examples from redirects:')\n",
    "print (redirects_data[10000:10002])\n",
    "print ('\\n')\n",
    "\n",
    "print ('The number of entries in pagelinks:', len(pagelinks_data))\n",
    "print ('A couple of examples from pagelinks:')\n",
    "print (pagelinks_data[100000:100002])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JLgMS37hw_O"
   },
   "source": [
    "#### <font color=\"red\">Note:</font> It is worth noting here that each article is uniquely represented by its URL, or rather, the last segment of its URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3A_oqICv4tYn"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEj1Gbc2JDn8"
   },
   "source": [
    "### Function `get_article_name` which takes as input the URL string, and extracts the article name from the url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_name(url):\n",
    "    prefix = \"<http://dbpedia.org/\"\n",
    "    prefix_index = url.find(prefix)\n",
    "    p1 = url.find(\"/\", prefix_index + len(prefix))\n",
    "    p2 = url.rindex(\">\")\n",
    "    article_name = url[p1+1:p2]\n",
    "    return article_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVxitX-eiGhX"
   },
   "source": [
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGVnitK0iASM"
   },
   "source": [
    "### Function `resolve_redirects` which takes as input a list of redirect lines, and returns a map between the initial and the resolved redirect page.\n",
    "\n",
    "e.g.: input = \\\n",
    "[\n",
    "'\\<http://dbpedia.org/resource/A> \\<http://dbpedia.org/property/redirect> \\<http://dbpedia.org/resource/B> .\\n',\\\n",
    "'\\<http://dbpedia.org/resource/B> \\<http://dbpedia.org/property/redirect> \\<http://dbpedia.org/resource/C> .\\n',\\\n",
    "'\\<http://dbpedia.org/resource/C> \\<http://dbpedia.org/property/redirect> \\<http://dbpedia.org/resource/D> .\\n',\\\n",
    "'\\<http://dbpedia.org/resource/X> \\<http://dbpedia.org/property/redirect> \\<http://dbpedia.org/resource/Z> .\\n'\n",
    "]\n",
    "\n",
    "output = {'A': 'D', 'B': 'D', 'C': 'D', 'X': 'Z'}\n",
    "\n",
    "#### <font color=\"red\">Note:</font> Ignoring malformed lines which are those which do not split in 4 parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6TyhO_2k3zhd"
   },
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "\n",
    "def resolve_redirects(redirect_lines):\n",
    "    \n",
    "    resolve_redirects = {}\n",
    "    for l, url in enumerate(redirect_lines):\n",
    "        split_url = url.split()\n",
    "        if len(split_url) != 4:\n",
    "            continue\n",
    "        resolve_redirects[get_article_name(split_url[0])] = get_article_name(split_url[2])\n",
    "\n",
    "    for to, from_ in enumerate(resolve_redirects.keys()):\n",
    "        tt = None\n",
    "        t = resolve_redirects[from_]\n",
    "        seen = {from_}\n",
    "        while True:\n",
    "            tt = t\n",
    "            t = resolve_redirects.get(t)\n",
    "            if t is None or t in seen:\n",
    "                break\n",
    "            seen.add(t)\n",
    "        resolve_redirects[from_] = tt\n",
    "\n",
    "    return resolve_redirects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HU0xRjui426w"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKI0kyPGJdgh"
   },
   "source": [
    "### Article-article adjacency matrix. \n",
    "### Let the number of articles $n$. The adjacency matrix should have a value $A[i][j]=1$ if there is a link from $i$ to $j$. Note that the matrix may not be symmetric. This matrix would have rows as source, and columns as destinations. However, for further sections, we need it the other way round. Therefore, return $A^\\top$ matrix. \n",
    "### Function `make_adjacency_matrix` that takes as input the resolved redirect map from above, and the list from `pagelinks_data`. Returns a tuple of `(index_map, A')`, where `index_map` is a map of each article to a unique number between $0$ and $n-1$, also its unique numerical id. `A` is the adjacency matrix in [scipy.sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html) format. `A'` is the transpose of matrix `A`. \n",
    "\n",
    "\n",
    "#### <font color=\"red\">Note:</font> If A redirects to D and X redirects to Y, and there is a pagelink entry from A to X, then the resolved pagelink entry should be D to Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "C9tE3opr4GOF"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def index_of(redirects, index_map, k):\n",
    "    k = redirects.get(k, k)\n",
    "    return index_map.setdefault(k, len(index_map))\n",
    "\n",
    "def make_adjacency_matrix(resolved_redirects, pagelinks_data):\n",
    "    index_map = dict()\n",
    "    links = list()\n",
    "    for l, url in enumerate(pagelinks_data):\n",
    "        split_url = url.split()\n",
    "        if len(split_url) != 4:\n",
    "            continue\n",
    "        i = index_of(resolved_redirects, index_map, get_article_name(split_url[0]))\n",
    "        j = index_of(resolved_redirects, index_map, get_article_name(split_url[2]))\n",
    "        links.append((i, j))\n",
    "\n",
    "    A = sparse.lil_matrix((len(index_map), len(index_map)), dtype=np.float32)\n",
    "    for i, j in links:\n",
    "        A[i, j] = 1.0\n",
    "\n",
    "    A = A.tocsr()\n",
    "    \n",
    "    return index_map,A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkATsaLd44Sz"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dADe7PA64Ups"
   },
   "source": [
    "### Applying the above functions on the dataset to create adjacency matrix $A$ and other relevant maps as directed below. Then applying `SVD` from sklearn on the adjacency matrix to determine principal singular vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SLWYY6b14UGB",
    "outputId": "4a1ba1e6-bf56-4fba-c647-4daec0391273",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. with redirects_data as input, use the resolve_redirects function to generate the redirects_map\n",
    "redirects_map = resolve_redirects(redirects_data)\n",
    "\n",
    "# 2. with redirects map from previous step pagelinks_data as inputs, use the make_adjacency_matrix to generate index_map and adjacency_matrix\n",
    "index_map, X = make_adjacency_matrix(redirects_map,pagelinks_data)\n",
    "\n",
    "# 3. using index_map, create a reverse_index_map, which has the article name as key, and its index as value\n",
    "reverse_index_map = dict((v,k) for k,v in index_map.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZPZghiu5iIY"
   },
   "source": [
    "### Applying ```randomized_svd``` from sklearn on the adjacency matrix. Extracting top 5 components and running for 3 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8pU8CY5v5C38",
    "outputId": "71d5c9fd-6115-4d08-82db-7f90281d18e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/extmath.py:368: FutureWarning: If 'random_state' is not supplied, the current default is to use 0 as a fixed seed. This will change to  None in version 1.2 leading to non-deterministic results that better reflect nature of the randomized_svd solver. If you want to silence this warning, set 'random_state' to an integer seed or to None explicitly depending if you want your code to be deterministic or not.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.extmath import randomized_svd\n",
    "U, s, V = randomized_svd(X,5,n_iter=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "07VFJzflIt2p",
    "outputId": "5d73afb3-320e-4f26-b702-fe85c264b2c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top wikipedia pages according to principal singular vectors\n",
      "[   'England',\n",
      "    'Spain',\n",
      "    'Italy',\n",
      "    'Canada',\n",
      "    'Japan',\n",
      "    'Germany',\n",
      "    'World_War_II',\n",
      "    'France',\n",
      "    'United_Kingdom',\n",
      "    'United_States']\n",
      "['1989', '1971', '1975', '1970', '2006', '1972', '1996', '1966', '1967', '2007']\n"
     ]
    }
   ],
   "source": [
    "# now, we print the names of the wikipedia related strongest components of the\n",
    "# principal singular vector which should be similar to the highest eigenvector\n",
    "print(\"Top wikipedia pages according to principal singular vectors\")\n",
    "pp.pprint([reverse_index_map[i] for i in np.abs(U.T[0]).argsort()[-10:]])\n",
    "pp.pprint([reverse_index_map[i] for i in np.abs(V[0]).argsort()[-10:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uBMcqfuKofd"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yc-kNfb_6cTe"
   },
   "source": [
    "## The Pagerank Algorithm \n",
    "### Implementing the google pagerank algorithm by computing principal eigenvector using power iteration method.\n",
    "\n",
    "### To start with the power iteration method, we first need to make the matrix $X$ obtained above *column stochastic*. A column stochastic matrix is a matrix in which each element represents a probability and the sum each column adds up to 1. Recall that $X$ is a matrix where the rows represent the destination and columns represents the source. The probability of visiting any destination from the source $s$ is $1/k$, where $k$ is the total number of outgoing links from $s$. Using this information to make the matrix column stochastic.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "42X9k-ow6bqY"
   },
   "outputs": [],
   "source": [
    "n = X.shape[0]\n",
    "X = X.copy()\n",
    "incoming_nodes = np.asarray(X.sum(axis=1)).ravel()\n",
    "\n",
    "for i in incoming_nodes.nonzero()[0]:\n",
    "    X.data[X.indptr[i] : X.indptr[i + 1]] *= 1.0 / incoming_nodes[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BaiGbHgO-1xI"
   },
   "source": [
    "### *Dangling Nodes*: There may exisit some pages which have no outgoing links. These are called as dangling nodes. If a random surfer just follows outgoing page links, then such a person can never leave a dangling node. We cannot just skip such a node, as there may be many pages pointing to this page, and could therefore be important. \n",
    "### To solve this problem, we introduce **teleportation** which says that a random surfer will visit an outgoing link with $\\beta$ probability and can randomly jump to some other page with a $(1-\\beta)/n$ probability (like through bookmarks, directly going through URL, etc.). Here $n$ is the total number of pages under consideration, and $\\beta$ is called the damping factor. So now, the modified transition matrix is:\n",
    "### $R = \\beta X + \\frac{(1-\\beta)}{n} I_{n\\times n}$\n",
    "\n",
    "### where $X$ is the column stochastic matrix from previous step, and $I_{n\\times n}$ is a $n\\times n$ identity matrix.\n",
    "\n",
    "### Using the transition matrix $R$, use the power iteration method to solve for the principal eigenvector $\\mathbf{p}_{n\\times 1}$. Start with an initial guess of $\\mathbf{p}_{n\\times 1}=[\\frac{1}{n}, \\frac{1}{n}, ..., \\frac{1}{n}]$, which intuitively represents that a random surfer can start at any page with a $\\frac{1}{n}$ probability. Use a damping factor of $0.85$, and perform a maximum of 100 iterations.\n",
    "\n",
    "### Reporting the top 10 page names which correspond to the top 10 scores (magnitudes) in the principal eigenvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cxLCTSisDUBJ"
   },
   "outputs": [],
   "source": [
    "max_iter = 100\n",
    "beta = 0.85\n",
    "tol = 1e-10\n",
    "dangle = np.asarray(np.where(np.isclose(X.sum(axis=1), 0), 1.0 / n, 0)).ravel()\n",
    "\n",
    "scores = np.full(n, 1.0 / n, dtype=np.float32)  # initial guess vector \n",
    "for i in range(max_iter):\n",
    "    prev_scores = scores\n",
    "    scores = (\n",
    "        beta * (scores * X + np.dot(dangle, prev_scores))\n",
    "        + (1 - beta) * prev_scores.sum() / n\n",
    "    )\n",
    "\n",
    "    scores_max = np.abs(scores).max()\n",
    "    if scores_max == 0.0:\n",
    "        scores_max = 1.0\n",
    "    err = np.abs(scores - prev_scores).max() / scores_max\n",
    "    if err < n * tol:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 scores and the corresponding pages\n",
      "Rank =  1 Page name =  Glasshouse\n",
      "Rank =  2 Page name =  Dark_Mansions\n",
      "Rank =  3 Page name =  Happy_Birthday_to_Me\n",
      "Rank =  4 Page name =  Skatetown%2C_U.S.A.\n",
      "Rank =  5 Page name =  James_at_15\n",
      "Rank =  6 Page name =  The_Loneliest_Runner\n",
      "Rank =  7 Page name =  Little_House_on_the_Prairie_%28film%29\n",
      "Rank =  8 Page name =  TP_de_Oro\n",
      "Rank =  9 Page name =  Which_Mother_Is_Mine%3F\n",
      "Rank =  10 Page name =  The_Egg\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 scores and the corresponding pages\")\n",
    "top_indexes = np.argsort(scores)[:10]\n",
    "rank=0\n",
    "for k in top_indexes:\n",
    "     if k in reverse_index_map:\n",
    "        rank +=1\n",
    "        print(\"Rank = \",rank,\"Page name = \",reverse_index_map[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNvW5iW4yJ2y"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
